---
title: "Code zur Masterarbeit"
author: "Xuan Son Le (4669361)"
date: "10/10/2018"
output: pdf_document: keep_tex: true
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)  
library(dplyr)
library(ggplot2)
library(xtable)
options(xtable.floating = FALSE)
options(xtable.timestamp = "")
library(Hmisc)
library(params)
```

```{r Daten importieren, warning=FALSE}
# CSV-Datei einlesen
dfCensus <- read.csv('c.csv')[,-1] #Spalte 1 enthält nur Zeilennummer
dfSurvey <- read.csv('s.csv')[,-1] #Spalte 1 enthält nur Zeilennummer

# Nur gemeinsame erklärende Variablen werden behalten
dfCensus <- dfCensus[,colnames(dfCensus) %in% colnames(dfSurvey)]
dfSurvey <- data.frame("cuadrantes" = dfSurvey[,"cuadrantes"],
                       dfSurvey[,colnames(dfSurvey) %in% colnames(dfCensus)])

# Deskriptive Statistik
#describe(dfSurvey)
#str(dfSurvey)
```

```{r Datenverständnis, warning = FALSE}
# Anteil an fehlenden Werten bestimmen und in einer Tabelle zusammenfassen
missingSurvey <- data.frame("Variable" = colnames(dfSurvey), "MissingSurvey" = t(data.frame(lapply(dfSurvey, function(x) round(100*sum(is.na(x))/length(x),1)))), row.names = NULL)

missingCensus <- data.frame("Variable" = colnames(dfCensus), "MissingCensus" = t(data.frame(lapply(dfCensus, function(x) round(100*sum(is.na(x))/length(x),1)))), row.names = NULL)

missingDF <- left_join(missingSurvey, missingCensus, by = 'Variable')
missingAll <- left_join(missingSurvey, missingCensus, by = 'Variable')

missingDF <- missingDF[which(missingDF[,2] > 10 | missingDF[,3] > 10),]
row.names(missingDF) <- c(1:nrow(missingDF))

# Latex-Formatierung #xtable(missingDF) 

# Variablen mit zu vielen fehlenden Werten werden entfernt
dfCensus <- dfCensus[, !(colnames(dfCensus) %in% missingDF[1:5,1])]
dfSurvey <- dfSurvey[, !(colnames(dfSurvey) %in% missingDF[1:5,1])]

# Variable ent wird entfernt, da nur ein Wert für alle Beobachtungen
dfCensus <- dfCensus[, -which(colnames(dfCensus) %in% c('ent'))]
dfSurvey <- dfSurvey[, -which(colnames(dfSurvey) %in% c('ent'))]

dfSurveyNew <- dfSurvey
dfCensusNew <- dfCensus

# Hilfsvariablen werden entfernt
hilfsvariablen <- c('est_dis', 'factor', 'idD', 'mun', 'upm')
dfSurveyNew <- dfSurveyNew[ , !(names(dfSurveyNew)  %in% hilfsvariablen)]
dfCensusNew <- dfCensusNew[ , !(names(dfCensusNew)  %in% hilfsvariablen)]

# Behandlung fehlender Werte
dfSurveyNew <- na.omit(dfSurveyNew)

# Multikorrelation überprüfen
library(caret)
correlationMatrix <- data.frame(cor(dfSurveyNew[,-1]))
colToRemove <- findCorrelation(cor(dfSurveyNew[,-1]), cutoff = 0.75)
dfSurveyNew <- dfSurveyNew[ , -(colToRemove + 1)]

# Transformation numerischer in nomialer Variablen. Ausgenommen sind edad und anos_escolaridad.
dfSurveyNew <- data.frame(lapply(dfSurveyNew[, -c(3,7)], function(x) factor(x)), 'edad' = dfSurveyNew[,3], 'anos_escolaridad' = dfSurveyNew[,7])
# dfCensusNew <- data.frame(lapply(dfCensus[, -c(3,7)], function(x) factor(x)), 'edad' = dfCensus[,3], 'anos_escolaridad' = dfCensus[,7])

# xtable(
#     aggregate(dfSurveyNew[,(names(dfSurveyNew) %in% c('edad', 'anos_escolaridad'))],
#               by = list(dfSurveyNew$cuadrantes),
#               FUN = median,
#               na.rm = TRUE,
#               na.action = na.pass)
# )

```

```{r Multimomiale logistische Regression}
# Datensatz in Trainings- und Testdatendatz splitten
require(caTools)
set.seed(123)
split = sample.split(dfSurveyNew$cuadrantes, SplitRatio = 0.75)
trainingSurvey = subset(dfSurveyNew, split == TRUE)
testSurvey = subset(dfSurveyNew, split == FALSE)

# multlogReg modellieren (anhand Trainingsdatensatz)
library(nnet)
fullMLR <- multinom(cuadrantes ~ ., data = trainingSurvey)
AICMLR <- step(fullMLR, direction = 'both')

#y_hat <- ifelse(y_prob < 0.5, 0 , 1 )

# Vorhersage (anhand Testdatensatz)
y_pred_MLR <- predict(AICMLR, newdata = testSurvey[-1])
#y_pred <- ifelse(y_pred < 0.5, 0 , 1 )

# Making the Confusion Matrix
confMat_MLR = table(testSurvey[,1], y_pred_MLR)

# Genauigkeit
accuracy_MLR <- sum(diag(confMat_MLR))/sum(confMat_MLR)
```


```{r SVM}
# Fitting SVM to the Training set
#install.packages("e1071")
library(e1071)
classifierSVM <- svm(formula = cuadrantes ~ .,
                  data = trainingSurvey,
                  type = "C-classification",
                  kernel = "sigmoid")

# Predicting the Test set results
y_pred_SVM = predict(classifierSVM, newdata = testSurvey[-1])

# Making the Confusion Matrix
confMat_SVM = table(testSurvey[,1], y_pred_SVM)

# Genauigkeit
accuracy_SVM <- sum(diag(confMat_SVM))/sum(confMat_SVM)
```




```{r eval=FALSE, include=FALSE}

# Visualising the Training set results
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = trainingSurvey
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
     main = 'Classifier (Training set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

# Visualising the Test set results
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'Classifier (Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```

